\documentclass[11pt,a4paper]{article}

\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{pdfpages}
\usepackage{gauss}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage[bottom]{footmisc}
\usepackage{algorithmicx}

% headers and footers
\usepackage{fancyhdr, lastpage}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\cfoot{Page \thepage\ of \pageref{LastPage}}

\title{Bachelor project \\
       \vspace{2mm}
       {\LARGE Efficient DNA/RNA-sequence clustering}}
\author{Anders Kiel Hovgaard \and Nikolaj Dybdahl Rathcke}

\begin{document}
\maketitle
\thispagestyle{empty}
\newpage

\begin{abstract}
  This will eventually contain a beautiful abstract...
\end{abstract}
\thispagestyle{plain}
\pagenumbering{roman}
\newpage

\tableofcontents
\thispagestyle{plain}
\newpage

\thispagestyle{fancy}
\pagenumbering{arabic}
\section{What should be in report?}

\begin{itemize}
  \item Introduction: introducing the motivation and goals for the project.
    
  \item Terminology: clarification of USEARCH vs UCLUST, definition of
    clustering and distance metric etc.

  \item Biology: the basics about gene sequences, DNA, RNA, sequencing etc.

  \item Background: theoretical overview of the field of clustering
    \begin{itemize}
      \item Types of cluster analysis algorithms
        \begin{itemize}
          \item Hierarchical clustering (agglomerative and divisive)
          \item Graph based clustering
          \item Gready algorithm like in \texttt{UCLUST}
          \item Gready algorithm with recalculation of centroids
        \end{itemize}

      \item Distance metrics
        \begin{itemize}
          \item Edit distance (Levenshtein)
          \item d2 distance (feature based distance)
          \item Sequence alignment
        \end{itemize}

      \item Clustering "history"
    \end{itemize}

  \item Research process
    \begin{itemize}
      \item Implementing basic Levenshtein and memoized Levenshtein,
        implementing d2 distance and comparing these.
      \item Testing USEARCH 32-bit on real data
      \item Implementing very (almost naively) gready clustering algorithm.
      \item Testing clustering algorithm with d2 distance and comparing
        performance to USEARCH.
    \end{itemize}
\end{itemize}


\section{To be done}
d2 - Remembering/reading kmers somehow instead of recomputing each time?


\section{Introduction}
Clustering is the task of grouping objects so that, based on a given similarity
threshold, each object belongs to only one cluster and the
objects within a cluster are more similar to each other than to objects 
in other clusters.

The main motivation for this project comes from a need for efficient tools for
clustering of sequence data, and related techniques, in the microbiology
department at the University of Copenhagen. In particular, the idea for this
project comes from a collaboration between the supervisor of this project,
Rasmus Fonseca, and Martin Asser Hansen, who is a bioinformatician in the
Molecular Microbial Ecology
Group\footnote{\url{http://www2.bio.ku.dk/microbiology/}}.

Clustering huge amounts of DNA/RNA-sequences (up to 500
million strings of 500-1500 characters each) is computationally hard and there are not many available algorithms and tools for efficient clustering of
sequencing data. The one tool
\texttt{UCLUST}\footnote{\url{http://drive5.com/usearch/}} that does the job is
closed-source and considered too expensive.

The goal of this project is to research the possibilities of creating an open source tool that can match the performance of the proprietary version of
\texttt{UCLUST}.


\section{Terminology}
This section describes some basic terminology and notation of this text.

The words \emph{sequence} and \emph{string} will be used to denote the same
concepts, i.e. a possibly infinite, ordered list of objects, where an object
will most often be a text character.

In this text, the notion of a subsequence is different from that of a
substring: a substring $S'$ of a sequence $S$ is a consecutive, ordered list of
objects, that occurs in $S$, while a subsequence $S''$ of $S$ is a sequence
that can be obtained from $S$ by deleting some objects from the sequence
without changing the order of the objects.

% Defintion of subsequence, what's a sequence, what's an alphabet, notation...

\subsection{Notation}
Let $s$ and $t$ be sequences.
\begin{itemize}
  \item $|s|$ denotes the length of $s$
  \item $s \sqsubseteq t$ denotes that $s$ is a substring of $t$
\end{itemize}


\section{Background}

\subsection{Distance metrics}

\subsubsection{Edit distance}
One type of \emph{edit distance} is the \emph{Levenshtein} distance, which is a
string metric for determining the similarity between two sequences. It is
defined to be the minimum number of edits to turn the first sequence into the
other. \\
The edit operations consists of \emph{insertions}, \emph{deletions} and
\emph{substitutions}. These operations are, respectively, inserting a letter,
removing a letter and changing one letter for another. \\
For example, the two sequences
\begin{center}
\texttt{ACGT} \\
\texttt{ACGGC}
\end{center}
would have a distance of 2 (substituting T for G and inserting a C). \\
However, there are some cases where the relevance of the distance is arguable.
Consider the sequences
\begin{center}
\texttt{AACC} \\
\texttt{CCAA}
\end{center}
with a distance of 4. The two sequences actually have the maximal distance
possible even though they have a close resemblance to each other.


\subsubsection{Feature based distance}
A $k$-mer, or $k$-gram or simply a \emph{word}, is a sequence of length
$k \geq 0$ over some alphabet $\mathcal{A}$ of a sequence. $k$-mers are a type
of sequence \emph{feature}. An interesting feature of a sequence is the
$k$-mers that occur in that sequence.

$d2$ is a feature based distance metric, using $k$-mers as the feature. The
distance is calculated by counting the $k$-mers occurring in two sequences,
representing these occurrences as two vectors and finally taking the Euclidean
distance between these two vectors.

Let $c_x(w)$ be the number of times that a $k$-mer $w$ occurs in the sequence
$x$. Then the $d2$ distance can be defined as follows:
\begin{equation}
  d2_k(x,y) = \sqrt{\sum_{|w|=k} (c_x(w) - c_y(w))^2}
\end{equation}

As an example, the two $2$-mer frequency vectors of the sequences
\begin{align*}
  S_1 &= AGACTG \\
  S_2 &= ACAGAT
\end{align*}
over the alphabet $\mathcal{A} = \{A,C,T,G\}$, can be illustrated as follows:

\begin{table}[!h]
\centering
\scalebox{0.7}{
\begin{tabular}{c | c c c c c c c c c c c c c c c c}
        & AA & AC & AG & AT & CA & CC & CG & CT & GA & GC & GG & GT & TA & TC & TG & TT \\
  \hline
  $S_1$ &    &  1 &  1 &    &    &    &    &  1 &  1 &    &    &    &    &    &  1 &    \\
  \hline
  $S_1$ &    &  1 &  1 &  1 &  1 &    &    &    &  1 &    &    &    &    &    &    &    \\
\end{tabular}}
\end{table}

The Euclidean distance would then be calculated as
\begin{align*}
  d2_2(S_1, S_2)
    &= \sqrt{(1-1)^2 + (1-1)^2 + (-1)^2 + (-1)^2 + (1)^2 + (1-1)^2 + (1)^2} \\
    &= 2
\end{align*}

To better support distance between two sequences of different length, but
similar subsequences, the concept of a \emph{window} can be used to split the
distance calculation into comparison of substrings of length of the window
and then use the window with the least distance as the resulting distance. In
this way two sequences, where one is a prefix or postfix of the other, will
have zero distance.




\subsubsection{Sequence alignment}
\emph{Sequence alignment} is used in bioinformatics to identify regions of
similarity by arranging two or more sequences in a certain manner. Besides
shifting a sequence to a side, gaps can be inserted between objects as well. We
represent the gap with '-' and it indicates a insertion in the sequence or a
deletion from another sequence.\\
We call a column that includes - for an indel. If it does not and all object in
the column are same, then we have a match. Otherwise, it is a mismatch.\\
\begin{figure}[h!]
  \centering
  	  \begin{align*}
  	  	\mbox{\texttt{ATGCA}} \\
      	\mbox{\texttt{-TGCG}}
  	  \end{align*}
  \caption{Sequence alignment of the sequences 'ATGCA' and 'TGCG'}
  \label{fig:seqAlignment}
\end{figure}
Figure \ref{fig:seqAlignment} displays an alignment of two sequences. Column 1
is an indel, column 2-4 are matches and column 5 is a mismatch. The amount of
each type can then be used in a formula to calculate the similarity between the
sequences. Note that more than two sequences can be aligned in a \emph{multiple
sequence alignment}. \\
There are many alignments. Among those is an optimal one which is what is
strived to be found.

\subsection{Cluster analysis algorithms}

\subsubsection{Hierarchical clustering}
(agglomerative and divisive)

\subsubsection{Graph based clustering}


\subsubsection{Greedy algorithm like in \texttt{UCLUST}} 


\subsubsection{Greedy algorithm with recalculation of centroids}


\section{Research process}
\subsection{Starting out: Levenshtein with dynamic programming}
For a start, a naive version of the Levenshtein distance metric was
implemented, but as expected, this is absolutely useless in any practical
setting, since it does not reuse already calculated results. Therefore, this
was quickly transformed into a dynamic programming solution, which solves
subproblems just once, stores and reuses the intermediate results.

\subsection{$d2$ distance}
After learning that the basic Levenshtein algorithm is far too slow for the
problem domain of this project, we turned our attention to the $d2$ distance
metric. The first edition of the $d2$ distance metric algorithm is shown in
figure \ref{alg:d2}. This algorithm maintains a single frequence vector, as a
map structure to allow for large $k$ which results in a large number of
possible different $k$-mers. The map is indexed using the lexicographical
position of the $k$-mer and when iterating through the first sequence, $k$-mer
counts are incremented and when iterating through the second sequence, they are
decremented. Finally all the frequensies are squared and the square root of the
sum is returned.

\begin{figure}[!h]
  \centering
  \begin{verbatim}
    d2(s,t, k):
      initialize freq_veq[i] to 0 for all i;

      for (int i = 0; i <= length(s) - k; i++)
        freq_vec[index_of_kmer(s.substr(i,k))]++;

      for (int i = 0; i <= length(t) - k; i++)
        freq_vec[index_of_kmer(s.substr(i,k))]--;

      map (^2) freq_vec;
      return sqrt (sum freq_vec);
  \end{verbatim}
  \caption{Initial $d2$ distance metric algorithm.}
  \label{alg:d2}
\end{figure}



\newpage
\begin{thebibliography}{9}
  \bibitem[Hazelhurst2003]{hazelhurst2003}
    Scott Hazelhurst,
    \emph{An implementation of the $d^2$ distance function for DNA
      sequences: The wcd $d^2$ EST clustering algorithm},
      September 2003,
      \url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.4289&rep=rep1&type=pdf}.

    \bibitem[Dong2007]{dong2007}
      Guozhu Dong, Jian Pei, \emph{Sequence Data Mining},
      Springer, 2007.
\end{thebibliography}



\end{document}
